Regenerating SQLx Offline Data
------------------------------
Purpose:
Keep each service's sqlx-data.json in sync with its queries + migrations so Docker builds (SQLX_OFFLINE=true) compile without a live database.

Script:
  powershell -ExecutionPolicy Bypass -File .\regenerate-sqlx-data.ps1

Defaults:
  - Uses DATABASE_URL (auto-sets to postgres://novapos:novapos@localhost:5432/novapos if unset)
  - Runs migrations (all selected services) then generates sqlx-data.json per service
  - Fails if any offline prepare fails

Common Scenarios:
1. Edit or add queries (no new migration):
   .\regenerate-sqlx-data.ps1 -Services customer-service
2. Add a new forward migration file:
   (create migration) -> .\regenerate-sqlx-data.ps1 -Services customer-service
3. Multiple services changed:
   .\regenerate-sqlx-data.ps1 -Services auth-service,order-service
4. Full workspace refresh:
   .\regenerate-sqlx-data.ps1
5. Checksum mismatch (rewrote old migration):
   .\regenerate-sqlx-data.ps1 -ResetDatabase
   (or auto) .\regenerate-sqlx-data.ps1 -AutoResetOnChecksum
6. Only regenerate offline metadata (skip migrations):
   .\regenerate-sqlx-data.ps1 -SkipMigrations
7. Only run migrations (no offline files needed yet):
   .\regenerate-sqlx-data.ps1 -SkipPrepare
8. Force clean DB even if healthy:
   .\regenerate-sqlx-data.ps1 -ResetDatabase


# Check for drift manually (simulates CI)
git add services/*/sqlx-data.json
git diff --cached --quiet
pwsh .\regenerate-sqlx-data.ps1
git add services/*/sqlx-data.json
git diff --cached   # should be empty


Flags:
  -Services <list>            Comma or array; defaults to all services
  -ResetDatabase              Drop & recreate DB before migrations
  -AutoResetOnChecksum        If checksum mismatch detected, automatically reset (no second run needed)
  -SkipMigrations             Do not execute sqlx migrate run
  -SkipPrepare                Skip cargo sqlx prepare (no sqlx-data.json updates)

When NOT to use -ResetDatabase:
  - Adding a migration at the end
  - Only changing Rust-side query logic or parameters

Indicators you DO need -ResetDatabase:
  - "checksum mismatch" warnings from the script
  - You edited or reordered an earlier migration
  - You intentionally want a clean local dataset

Result Artifacts:
  services/<service>/sqlx-data.json (commit these changes with migrations + query edits)

Typical Workflow (single service change):
  1. Modify queries / add migration
  2. .\regenerate-sqlx-data.ps1 -Services customer-service
  3. git add services/customer-service/migrations/* services/customer-service/sqlx-data.json
  4. docker compose build customer-service && docker compose up -d customer-service

Troubleshooting:
  - Missing sqlx-data.json during Docker build: re-run script for that service
  - Prepare failure: check the first compile error; usually a missing feature flag or unresolved type
  - Persistent checksum mismatch after reset: ensure old migration file matches DB state or recreate DB again

(Do not embed the script’s PowerShell source here—see regenerate-sqlx-data.ps1 for implementation.)
HashiCorp Vault (local dev)



Unified HTTP Error Envelope & Metrics (Oct 2025)
------------------------------------------------
Scope:
All HTTP microservices (product, inventory, order, payment, loyalty, customer, integration-gateway, auth where applicable) now emit a standardized error shape and header plus a Prometheus counter when a non-2xx response is generated by application code.

Error JSON Shape:
{
  "code": "<error_code>",
  "missing_role": "<role>"                # only for missing role (optional)
  "trace_id": "<uuid>"                   # present when security context / extractor provided one
  "message": "<debug/human message>"     # only on some internal or explicit bad_request cases
}

HTTP Header:
X-Error-Code: <error_code> (mirrors body.code)

Primary Variants (common-http-errors crate ApiError enum):
- missing_role -> 403 with missing_role field populated
- forbidden -> 403
- <custom bad_request code> -> 400 (e.g. invalid_customer_id, invalid_order_id, etc.)
- <custom not_found code> -> 404 (e.g. product_not_found)
- internal_error -> 500 (includes optional message; scrub before exposing sensitive info)

Rust Type:
Result<T, ApiError> exported as ApiResult<T> in handlers.

Adding a new error code:
1. Prefer existing variants; for new 400/404 codes, use ApiError::bad_request("new_code", trace_id) or ApiError::NotFound { code: "new_code", trace_id }.
2. Keep codes snake_case, stable, and consumer-safe (no spaces, no PII).
3. If adding a wholly new variant (rare), modify `services/common/http-errors/src/lib.rs` and add a regression test in that crate or service-level error_shape test.

Trace IDs:
- Provided by SecurityCtxExtractor rollout (TA-ROL-*). If missing, trace_id may be null/absent. Future improvement: enforce presence post-rollout completion.

Metrics:
- Counter: http_errors_total{service,code,status}
  * Incremented by middleware when an ApiError response is produced.
  * service label is static per binary.
  * code matches the body.code and X-Error-Code header.
  * status is the HTTP status code (string form).

Dashboards / Alerting (planned follow-ons):
- Add rate of 5xx grouped by service (internal_error)
- Watch cardinality of code label; avoid embedding dynamic IDs.

Local Verification:
PowerShell example (trigger 404):
Invoke-WebRequest -Uri http://localhost:8081/products/00000000-0000-0000-0000-000000000000 -Method GET -Headers @{"Authorization"="Bearer <token>";"X-Tenant-ID"="<tenant>"} -ErrorAction SilentlyContinue | Select-Object -ExpandProperty Content
curl http://localhost:8081/metrics | findstr http_errors_total

Expected body snippet: {"code":"product_not_found","trace_id":"<uuid>"}
Expected metrics line contains: http_errors_total{service="product-service",code="product_not_found",status="404"} 1

Operational Playbook:
1. If clients report generic failures, ask for X-Error-Code + trace_id to correlate logs.
2. Spike in internal_error for a service -> inspect recent deploy, check logs filtered by trace_id values.
3. Missing X-Error-Code header implies the response didn't go through ApiError (could be middleware / framework). Add mapping or wrap.
4. No metrics increment but error returned: ensure handler returns ApiError (not raw StatusCode/String).

Troubleshooting Matrix:
- Symptom: 403 but code != missing_role and no missing_role field -> ApiError::Forbidden path. Confirm roles & extractor output.
- Symptom: 500 without internal_error code -> unwrap/panic or framework-level error. Add catch and map to ApiError::Internal.
- Symptom: High variety of bad_request codes -> review for embedding data; normalize to a finite vocabulary.

Upcoming Related Tasks:
- TA-OPS-4: central helper to register http_errors_total to remove per-service duplication & add cardinality guard.
- TA-OPS-5: standardized regression tests to ensure envelope + metrics increments for representative variants.
- TA-FND-5: add kafka feature gating to integration-gateway for Windows/local build parity.

Do NOT expose sensitive internal DB errors—wrap them with ApiError::internal(e, trace_id) which stringifies e (sanitize if needed before stable release).

Adoption Checklist (new service):
- Add dependency: common-http-errors
- Replace handler signatures to use ApiResult<T>
- Insert middleware that intercepts responses and increments http_errors_total on ApiError (replicate from any migrated service until TA-OPS-4 is done)
- Ensure /metrics endpoint is mounted.


patterns...to create topics



------------------------------
- docker compose up -d vault
- export VAULT_ADDR="http://127.0.0.1:8200" (PowerShell: $env:VAULT_ADDR="http://127.0.0.1:8200")
- export VAULT_TOKEN="root" (PowerShell: $env:VAULT_TOKEN="root")
- vault kv put secret/novapos/auth \
    KAFKA_BOOTSTRAP="kafka:9092" \
    REDIS_URL="redis://redis:6379/0" \
    SECURITY_MFA_ACTIVITY_TOPIC="security.mfa.activity"
- vault kv put secret/novapos/integration-gateway \
    SECURITY_ALERT_TOPIC="security.alerts.v1"
- vault kv get secret/novapos/auth



Full Docker Build including app

docker compose build

docker compose up -d

docker compose up kafka-topics-init

// tear down..... docker compose down
// docker compose restart



//If you need to rebuild just one service after code changes:

docker compose up -d --build auth-service

docker compose up -d --build order-service

docker compose up -d --build loyalty-service

docker compose up -d --build customer-service

docker compose up -d --build product-service

docker compose ps confirms all services  are running.


docker compose exec kafka kafka-topics.sh --create --topic payment.completed --bootstrap-server kafka:9092 --replication-factor 1 --partitions 1

docker compose exec kafka kafka-topics.sh --create --topic payment.failed --bootstrap-server kafka:9092 --replication-factor 1 --partitions 1

docker compose exec kafka kafka-topics.sh --create --topic order.completed --bootstrap-server kafka:9092 --replication-factor 1 --partitions 1



You???re in the repo root, but the Cargo workspace is under services/. Use one of these:





Run docker compose ps so we can confirm which services are stopped/exited.



Inspect the logs for one of the failing services (e.g. docker compose logs auth-service); that usually shows missing env vars or migration errors.



If everything is down, docker compose logs without a service name will dump them all.





Option B: Use the workspace manifest path from root





**Local build to docker only

cargo run --manifest-path services/Cargo.toml -p product-service

cargo run --manifest-path services/Cargo.toml -p analytics-service

cargo run --manifest-path services/Cargo.toml -p integration-gateway

cargo run --manifest-path services/Cargo.toml -p order-service

cargo run --manifest-path services/Cargo.toml -p auth-service

cargo run --manifest-path services/Cargo.toml -p payment-service

cargo run --manifest-path services/Cargo.toml -p loyalty-service

cargo run --manifest-path services/Cargo.toml -p customer-service







List containers on novapos_novanet:

docker network inspect novapos_novanet

??? check the Containers block to see which services are connected.





Now that you know which containers sit on novapos_novanet, grab their service names/IDs from the Containers block and stream each one???s logs.



Tail a single container (replace with the ID or name you copied, e.g. novapos-order-service-1):

docker logs -f --tail 200 novapos-order-service-1



If you prefer the compose aliases (you???re in C:\Projects\novapos):

 



Need everything at once?

docker compose logs -f



docker compose logs -f order-service

docker compose logs -f integration-gateway

docker compose logs -f payment-service







See the services (health endpoints)



product: curl http://localhost:8081/healthz

analytics: curl http://localhost:8082/healthz

gateway: curl http://localhost:8083/healthz

order: curl http://localhost:8084/healthz

auth: curl http://localhost:8085/healthz

payment: curl http://localhost:8086/healthz

loyalty: curl http://localhost:8088/healthz

customer: curl http://localhost:8089/healthz

PowerShell alt: irm http://localhost:8081/healthz







Quick Start



Makefile task: run .\Makefile.ps1 Dev-Frontend -App pos-app from the repo root.

Alt (if policy blocks scripts): powershell -ExecutionPolicy Bypass -File .\Makefile.ps1 Dev-Frontend -App pos-app

Open: http://localhost:5173

Stop: press Ctrl+C in the terminal running the dev server.

Manual Start



Go to app: cd frontends/pos-app

Install deps: npm install

Run Vite: npm run dev



C:\Projects\novapos\frontends\admin-portal> npm run dev

 C:\Projects\novapos\frontends\pos-app> npm run dev





**********************************postgres in docker*********************************************************

PS C:\Projects\novapos> docker run --name novapos-pg -e POSTGRES_PASSWORD=postgres -p 5432:5432 -d postgres:16



PS C:\Projects\novapos> docker exec -it novapos-pg psql -U postgres -c "CREATE DATABASE novapos;"

 



If you want to stay connected, open psql against the default postgres database and kill the stragglers manually:

psql postgres://novapos:novapos@localhost:5432/postgres



then inside psql run:

SELECT pg_terminate_backend(pid)

FROM pg_stat_activity

WHERE datname = 'novapos'

  AND pid <> pg_backend_pid();

That terminates every session using novapos. E







Use this same URL for product-service, order-service, auth-service, and inventory-service in the prototype:



# For current shell session:

$env:DATABASE_URL = "postgres://novapos:novapos@localhost:5432/novapos"





# Make it persistent for future shells and on this 

setx DATABASE_URL "postgres://novapos:novapos@localhost:5432/novapos



sqlx database drop -y -D 'postgres://novapos:novapos@localhost:5432/novapos'

sqlx database create -D 'postgres://novapos:novapos@localhost:5432/novapos'

powershell -ExecutionPolicy Bypass -File .\migrate-all.ps1



***************MANUAL Drop and Recreate DATABASE

PS C:\Projects\novapos> psql -U novapos -d novapos -h localhost -p 5432

Password for user novapos: 



psql (16.10)

WARNING: Console code page (437) differs from Windows code page (1252)

         8-bit characters might not work correctly. See psql reference

         page "Notes for Windows users" for details.

Type "help" for help.



novapos=# DROP DATABASE IF EXISTS novapos;

ERROR:  cannot drop the currently open database

novapos=# \c postgres

You are now connected to database "postgres" as user "novapos".

postgres=# DROP DATABASE IF EXISTS novapos;

DROP DATABASE

postgres=# CREATE DATABASE novapos OWNER novapos;

CREATE DATABASE





cd services\product-service

sqlx database create     # safe to re-run

sqlx migrate run --ignore-missing



cd ..\order-service

sqlx migrate run --ignore-missing



cd ..\auth-service

sqlx migrate run --ignore-missing



cd ..\inventory-service

sqlx migrate run --ignore-missing



cd ..\customer-service

sqlx migrate run --ignore-missing



cd ..\loyalty-service

sqlx migrate run --ignore-missing



cd ..\analytics-service

sqlx migrate run --ignore-missing



cd ..\integration-gateway

sqlx migrate run --ignore-missing



cd ..\payment-service

sqlx migrate run --ignore-missing









Yes, if you use the setx command, the environment variable will be set persistently for your user account. It will remain available in all new PowerShell or Command Prompt sessions???even after restarting your computer or server.



However, note:



It will not be available in already-open shells; you must open a new shell to see the change.

It is set for your user, not system-wide (unless you use /M for machine-wide).



*******************************************************************************************



Default POS credentials (from auth-service migration 3002)

Email: admin@novapos.local

Password: admin123

Role: super_admin (tenant NovaPOS HQ)







#Run from elvate enviroment

choco install openssl.light 



Step 1. Generate a Dev RSA Key



Run this once in PowerShell:



# Generate a 2048-bit RSA private key (requires OpenSSL installed)

openssl genrsa -out jwt-dev.pem 2048



# Extract public key (optional, useful for verifying tokens)

openssl rsa -in jwt-dev.pem -pubout -out jwt-dev.pub.pem



# Windows PowerShell ---docker compose gives it to services

$env:JWT_DEV_PRIVATE_KEY_PEM = Get-Content jwt-dev.pem -Raw







Tenant & Key Management Updates (Sept 2025)

------------------------------------------

- After pulling latest changes run auth-service migrations 3003/3004 (e.g. sqlx migrate run) to create the integration_keys table and role constraint.

- integration-gateway now expects DATABASE_URL (same Postgres DSN as other services) and optionally KEY_REFRESH_SECONDS (default 60). Remove any tenant_config.json usage.

- Generate/rotate integration keys via POST /tenants/{tenant_id}/integration-keys (or Admin UI > Settings); broadcast new key to external systems and revoke unused ones.

- Admin portal linting now uses eslint.config.js (flat config). Run npm run lint once to ensure the new config is picked up.



Set CUSTOMER_MASTER_KEY before running customer-service (base64-encoded 32-byte key).

Example: $env:CUSTOMER_MASTER_KEY = "wJr6c+g4vF5n3fH0wIr1Vj0pND+1tQgxOcnANLmJHzk="

Seed tenant data keys (from repo root):

cargo run --manifest-path services/Cargo.toml -p customer-service --bin seed_tenant_keys -- --tenant <TENANT_UUID>

Add --rotate to force a new version when one already exists.

Apply customer-service migrations (ensure DATABASE_URL points to target env):

sqlx migrate run --source services/customer-service/migrations --ignore-missing



Backfill customer PII columns (defaults batch=100):

cargo run --manifest-path services/Cargo.toml -p customer-service --bin backfill_customer_pii -- --tenant <TENANT_UUID>

Add --dry-run to report counts without updating rows.











----------------------------Here???s how to run the operational side of the PII/GDPR work with the stack 

 1. Confirm the tables exist

 docker compose exec -T postgres psql -U novapos -d novapos -c "\dt auth_signing_keys" ; docker compose exec -T postgres psql -U novapos -d novapos -c "\dt auth_refresh_tokens" ; docker compose exec -T postgres psql -U novapos -d novapos -c "\dt tenant_data_keys" ; docker compose exec -T postgres psql -U novapos -d novapos -c "\dt gdpr_tombstones"



 2. Pick a Tenant to Seed DEKs

 docker compose exec -T postgres psql -U novapos -d novapos -c "SELECT id, name FROM tenants;"



 3. Then seed a key for each tenant (dry-run first if you like; no DB change happens when the tenant doesn???t exist)

# Set the DB URL for these commands

$env:DATABASE_URL = "postgres://novapos:novapos@localhost:5432/novapos"

$tenant = "3d9adee5-e09f-4c60-9e6c-4e6c998ec986"

$env:CUSTOMER_MASTER_KEY = "oDzFP29jTB7sNhBJCaVRJQvTIhhPCi+WuSSmdPxGNIs="







# seed the tenant key

cargo run --release --manifest-path services/Cargo.toml `

  -p customer-service --bin seed_tenant_keys `

  -- --tenant $tenant



  Backfill or create a customer then backfill that for the tenant

# dry-run the backfill to check counts...Dry run to see how many customers need work:

cargo run --release --manifest-path services/Cargo.toml `

  -p customer-service --bin backfill_customer_pii `

  -- --tenant $tenant --dry-run



#Confirm the UUID:



docker compose exec -T postgres psql -U novapos -d novapos `

  -c "SELECT id, tenant_id FROM customers;"



4. Exercise the GDPR endpoints



 Call the GDPR export endpoint



 Get and Authorization for that user for a tenant



$headers = @{ "Content-Type" = "application/json" }

$body = '{"email":"jbean@bfurniture.com","password":"admin123"}' 



$response = Invoke-WebRequest -Uri "http://localhost:8085/login" -Method POST -Headers $headers -Body $body

# Parse the JSON and display the full token

($response.Content | ConvertFrom-Json).token



Verify the tombstone

docker compose exec -T postgres psql -U novapos -d novapos `

  -c "SELECT id, tenant_id, request_type, status, metadata FROM gdpr_tombstones ORDER BY requested_at DESC LIMIT 5;"



5. Call GDPR delete

Invoke-WebRequest -Uri "http://localhost:8089/customers/<customer_id>/gdpr/delete" `

  -Method POST `

  -Headers @{

      "Authorization" = "Bearer <access_token>"

      "X-Tenant-ID"   = "<TENANT_UUID>"

  }



6. docker compose exec -T postgres psql -U novapos -d novapos `

  -c "SELECT name, email, phone, email_encrypted, phone_encrypted FROM customers WHERE id = '<customer_id>';"






***********************************************************************************************
Auth-Service Metrics Smoke
--------------------------
# Export once per shell when testing locally
$env:KAFKA_BOOTSTRAP = "localhost:9092"
$env:SECURITY_MFA_ACTIVITY_TOPIC = "security.mfa.activity"
$env:SECURITY_SUSPICIOUS_WEBHOOK_URL = ""
$env:SECURITY_SUSPICIOUS_WEBHOOK_BEARER = ""

# Start the auth stack (postgres + kafka already seeded by compose)
docker compose up -d postgres kafka redis
docker compose up -d --build auth-service

#  make sure admin has an MFA secret so the login flow enforces a code
docker compose exec -T postgres psql -U novapos -d novapos -c "UPDATE users SET mfa_secret='JBSWY3DPEHPK3PXP', mfa_enrolled_at=NOW(), mfa_failed_attempts=0 WHERE email='admin@novapos.local';"

# Generate a current TOTP code for that secret (run right before you post /login) ---only last 30 seconds
python - <<'PY'
import base64, hashlib, hmac, struct, time
secret = 'JBSWY3DPEHPK3PXP'
key = base64.b32decode(secret, casefold=True)
counter = int(time.time()) // 30
msg = struct.pack('>Q', counter)
digest = hmac.new(key, msg, hashlib.sha1).digest()
offset = digest[-1] & 0x0F
code = (struct.unpack('>I', digest[offset:offset+4])[0] & 0x7FFFFFFF) % 1_000_000
print(f"{code:06d}")
PY

# Drive the telemetry (replace <TENANT_ID> with the admin tenant GUID)
$tenant = "<TENANT_ID>"
$headers = @{ "Content-Type" = "application/json"; "X-Tenant-ID" = $tenant }

# 1. Invalid credentials -> auth_login_attempts_total{outcome="invalid_credentials"}
$badBody = '{"email":"admin@novapos.local","password":"wrong"}'
Invoke-WebRequest -UseBasicParsing -Uri "http://localhost:8085/login" -Method POST -Headers $headers -Body $badBody

# 2. Missing MFA code -> auth_login_attempts_total{outcome="mfa_required"}
$noMfaBody = '{"email":"admin@novapos.local","password":"admin123"}'
Invoke-WebRequest -UseBasicParsing -Uri "http://localhost:8085/login" -Method POST -Headers $headers -Body $noMfaBody

# 3. Bad MFA code -> auth_login_attempts_total{outcome="mfa_invalid"}
$badMfaBody = '{"email":"admin@novapos.local","password":"admin123","mfaCode":"000000"}'
Invoke-WebRequest -UseBasicParsing -Uri "http://localhost:8085/login" -Method POST -Headers $headers -Body $badMfaBody

# 4. Happy path -> increments auth_login_attempts_total{outcome="success"} and auth_mfa_events_total labels
$goodCode = Read-Host "Enter current TOTP"
$goodBody = '{"email":"admin@novapos.local","password":"admin123","mfaCode":"' + $goodCode + '"}'
Invoke-WebRequest -UseBasicParsing -Uri "http://localhost:8085/login" -Method POST -Headers $headers -Body $goodBody | ConvertFrom-Json

# Inspect counters
curl http://localhost:8085/metrics | findstr auth_login_attempts_total
curl http://localhost:8085/metrics | findstr auth_mfa_events_total

Integration-Gateway Metrics Smoke
---------------------------------
# Optional webhook vars (leave blank if you just want Prometheus metrics)
$env:SECURITY_ALERT_TOPIC = "security.alerts.v1"
$env:SECURITY_ALERT_WEBHOOK_URL = ""
$env:SECURITY_ALERT_WEBHOOK_BEARER = ""
$env:REDIS_URL = "redis://localhost:6379/0"
# Order-service prereq: set INVENTORY_SERVICE_URL so the gateway/order sync hits the internal host instead of localhost
# Compose example -> under order-service.environment add:
#   - INVENTORY_SERVICE_URL=http://inventory-service:8087
# Without this the POS queue stays offline with "Failed to contact inventory-service" errors.

docker compose up -d redis postgres kafka
docker compose up -d --build integration-gateway

# Hit the gateway a few times to record rate checks / rejections

Generate a current TOTP code 


$headers = @{ "Content-Type" = "application/json" }
$goodCode = "255362"
$body = '{"email":"admin@novapos.local","password":"admin123","mfaCode":"'+$goodCode+'"}'
$response = Invoke-WebRequest -Uri "http://localhost:8085/login" -Method POST -Headers $headers -Body $body
($response.Content | ConvertFrom-Json).token

$token = "eyJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiIsImtpZCI6ImxvY2FsLWRldiJ9.eyJzdWIiOiIwMDAwMDAwMC0wMDAwLTAwMDAtMDAwMC0wMDAwMDAwMDAxMDEiLCJ0aWQiOiIwMDAwMDAwMC0wMDAwLTAwMDAtMDAwMC0wMDAwMDAwMDAwMDEiLCJyb2xlcyI6WyJzdXBlcl9hZG1pbiJdLCJpc3MiOiJodHRwczovL2F1dGgubm92YXBvcy5sb2NhbCIsImF1ZCI6Im5vdmFwb3MtZnJvbnRlbmQsbm92YXBvcy1hZG1pbixub3ZhcG9zLXBvc3RncmVzIiwiZXhwIjoxNzU4NTgxNjAyLCJpYXQiOjE3NTg1ODA3MDIsImp0aSI6ImQxZDI5ZDhlLTNhOGQtNDYzOC1iNTg4LTJhNmJmMDgzODk0NyJ9.D-GSkAye8b_paiWK_bfsDoHHClJ_a_rN3lPsmMSvAvcv7utlPmUx6Pp064LcQQNYV8fQXk48lwrrYtM59FA3oPPySiJbbFniLXewNaWmUrEFA8wtBNNh8qWiOqorDpSzmZ8qoKLzocPqowkKqq7gsshUFfht1M8jpgQefUWw8fskmdmhDq_gNvMfbvPZphDQopRkTDVeXYo4FvIvHfUOMjaPuIdvuyC3WAXJAJ81o0WZ8rV-M5EyC52Hf7bB0mfQk9q1wHWw9xkquEOBmAKTQ5g46l5C89bztbceB5XxrZxOewVyQA5sMmHLUvLNVRvWVx_osZULjfKCpPe9ZZpbXw"
$headers = @{ "Authorization" = "Bearer $token" }
Invoke-WebRequest -Uri "http://localhost:8083/metrics" -Headers $headers

Expect counters like gateway_rate_limit_total{outcome="allowed"} and possibly "rejected" if limits were crossed.


